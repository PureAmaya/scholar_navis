'''
Original Author: gpt_academic@binary-husky

Modified by PureAmaya on 2025-03-06
- Remove unnecessary print.
- Due to Alibaba Cloud's introduction of a new inference model, the Qwen series models have been moved here. User-friendly names have also been added for distinction.

Modified by PureAmaya on 2025-02-21
- Compatible with DeepSeek's inference model(R1)

Modified by PureAmaya on 2024-12-28
- Compatible with custom API functionality on the web.
- Add localization support.
'''

import json
import time
import logging
import traceback
import requests
from shared_utils.config_loader import get_conf
from multi_language import init_language
from dependencies.i18n.gradio_i18n import HTML
from shared_utils.advanced_markdown_format import md2html

_ = init_language

# config_private.pyæ”¾è‡ªå·±çš„ç§˜å¯†å¦‚APIå’Œä»£ç†ç½‘å€
# è¯»å–æ—¶é¦–å…ˆçœ‹æ˜¯å¦å­˜åœ¨ç§å¯†çš„config_privateé…ç½®æ–‡ä»¶ï¼ˆä¸å—gitç®¡æ§ï¼‰ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¦†ç›–åŸconfigæ–‡ä»¶
from toolbox import (
    update_ui,
    is_the_upload_folder,
)

proxies, TIMEOUT_SECONDS, MAX_RETRY = get_conf(
    "proxies", "TIMEOUT_SECONDS", "MAX_RETRY"
)

timeout_bot_msg = (
    "[Local Message] Request timeout. Network error. Please check proxy settings in config.py."
    + "ç½‘ç»œé”™è¯¯ï¼Œæ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦å¯ç”¨ï¼Œä»¥åŠä»£ç†è®¾ç½®çš„æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæ ¼å¼é¡»æ˜¯[åè®®]://[åœ°å€]:[ç«¯å£]ï¼Œç¼ºä¸€ä¸å¯ã€‚"
)


def get_full_error(chunk, stream_response):
    """
    å°è¯•è·å–å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
    """
    while True:
        try:
            chunk += next(stream_response)
        except:
            break
    return chunk


def decode_chunk(chunk):
    """
    ç”¨äºè§£è¯»"content"å’Œ"finish_reason"çš„å†…å®¹
    """
    chunk = chunk.decode()
    response = ""
    reasoning_content = ""
    finish_reason = "False"
    try:
        chunk = json.loads(chunk[6:])
    except:
        response = ""
        finish_reason = chunk
    # é”™è¯¯å¤„ç†éƒ¨åˆ†
    if "error" in chunk:
        response = "API_ERROR"
        try:
            chunk = json.loads(chunk)
            finish_reason = chunk["error"]["code"]
        except:
            finish_reason = "API_ERROR"
        return response, reasoning_content, finish_reason

    try:
        response = chunk["choices"][0]["delta"]["content"]
    except:
        pass
    try:
        reasoning_content = chunk["choices"][0]["delta"]["reasoning_content"]
    except:
        pass
    try:
        finish_reason = chunk["choices"][0]["finish_reason"]
    except:
        pass

    if not response:response = ""
    if not reasoning_content:reasoning_content = ""

    return response, reasoning_content, finish_reason


def generate_message(input, model, key, history, max_output_token, system_prompt, temperature):
    """
    æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œé€‰æ‹©LLMæ¨¡å‹ï¼Œç”Ÿæˆhttpè¯·æ±‚ï¼Œä¸ºå‘é€è¯·æ±‚åšå‡†å¤‡
    """
    api_key = f"Bearer {key}"

    headers = {"Content-Type": "application/json", "Authorization": api_key}

    conversation_cnt = len(history) // 2

    messages = [{"role": "system", "content": system_prompt}]
    if conversation_cnt:
        for index in range(0, 2 * conversation_cnt, 2):
            what_i_have_asked = {}
            what_i_have_asked["role"] = "user"
            what_i_have_asked["content"] = history[index]
            what_gpt_answer = {}
            what_gpt_answer["role"] = "assistant"
            what_gpt_answer["content"] = history[index + 1]
            if what_i_have_asked["content"] != "":
                if what_gpt_answer["content"] == "":
                    continue
                if what_gpt_answer["content"] == timeout_bot_msg:
                    continue
                messages.append(what_i_have_asked)
                messages.append(what_gpt_answer)
            else:
                messages[-1]["content"] = what_gpt_answer["content"]
    what_i_ask_now = {}
    what_i_ask_now["role"] = "user"
    what_i_ask_now["content"] = input
    messages.append(what_i_ask_now)
    playload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "stream": True,
        "max_tokens": max_output_token,
    }

    return headers, playload


def get_predict_function(
        api_key_conf_name,
        friendly_name,
        max_output_token,
        disable_proxy = False
    ):
    """
    ä¸ºopenaiæ ¼å¼çš„APIç”Ÿæˆå“åº”å‡½æ•°ï¼Œå…¶ä¸­ä¼ å…¥å‚æ•°ï¼š
    api_key_conf_nameï¼š
        `config.py`ä¸­æ­¤æ¨¡å‹çš„APIKEYçš„åå­—ï¼Œä¾‹å¦‚"YIMODEL_API_KEY"
    max_output_tokenï¼š
        æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§tokenæ•°é‡ï¼Œä¾‹å¦‚å¯¹äº01ä¸‡ç‰©çš„yi-34b-chat-200kï¼Œå…¶æœ€å¤§è¯·æ±‚æ•°ä¸º4096
        âš ï¸è¯·ä¸è¦ä¸æ¨¡å‹çš„æœ€å¤§tokenæ•°é‡ç›¸æ··æ·†ã€‚
    disable_proxyï¼š
        æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼ŒTrueä¸ºä¸ä½¿ç”¨ï¼ŒFalseä¸ºä½¿ç”¨ã€‚
    """

    def predict_no_ui_long_connection(
        inputs,
        llm_kwargs,
        history=[],
        sys_prompt="",
        observe_window=None,
        console_slience=True,
    ):
        """
        å‘é€è‡³chatGPTï¼Œç­‰å¾…å›å¤ï¼Œä¸€æ¬¡æ€§å®Œæˆï¼Œä¸æ˜¾ç¤ºä¸­é—´è¿‡ç¨‹ã€‚ä½†å†…éƒ¨ç”¨streamçš„æ–¹æ³•é¿å…ä¸­é€”ç½‘çº¿è¢«æã€‚
        inputsï¼š
            æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
        sys_prompt:
            ç³»ç»Ÿé™é»˜prompt
        llm_kwargsï¼š
            chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
        historyï¼š
            æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨
        observe_window = Noneï¼š
            ç”¨äºè´Ÿè´£è·¨è¶Šçº¿ç¨‹ä¼ é€’å·²ç»è¾“å‡ºçš„éƒ¨åˆ†ï¼Œå¤§éƒ¨åˆ†æ—¶å€™ä»…ä»…ä¸ºäº†fancyçš„è§†è§‰æ•ˆæœï¼Œç•™ç©ºå³å¯ã€‚observe_window[0]ï¼šè§‚æµ‹çª—ã€‚observe_window[1]ï¼šçœ‹é—¨ç‹—
        """
        APIKEY = llm_kwargs['custom_api_key'](api_key_conf_name)
        watch_dog_patience = 5  # çœ‹é—¨ç‹—çš„è€å¿ƒï¼Œè®¾ç½®5ç§’ä¸å‡†å’¬äºº(å’¬çš„ä¹Ÿä¸æ˜¯äºº
        if len(APIKEY) == 0:
            raise RuntimeError(_("APIKEYä¸ºç©º,è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„{}ã€‚æˆ–è€…å¯ä»¥è‡ªå®šä¹‰ {} API-KEY").format(api_key_conf_name,friendly_name))
        if inputs == "":
            inputs = "ä½ å¥½ğŸ‘‹"
        headers, playload = generate_message(
            input=inputs,
            model=llm_kwargs["llm_model"],
            key=APIKEY,
            history=history,
            max_output_token=max_output_token,
            system_prompt=sys_prompt,
            temperature=llm_kwargs["temperature"],
        )
        retry = 0
        while True:
            try:
                from .bridge_all import model_info

                endpoint = model_info[llm_kwargs["llm_model"]]["endpoint"]
                if not disable_proxy:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        proxies=proxies,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                else:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                break
            except:
                retry += 1
                traceback.print_exc()
                if retry > MAX_RETRY:
                    raise TimeoutError
                if MAX_RETRY != 0:
                    print(f"{_('è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•')} ({retry}/{MAX_RETRY}) â€¦â€¦")

        stream_response = response.iter_lines()
        result = ""
        finish_reason = ""
        while True:
            try:
                chunk = next(stream_response)
            except StopIteration:
                if result == "":
                    raise RuntimeError(f"{_('è·å¾—ç©ºçš„å›å¤ï¼Œå¯èƒ½åŸå› : ')}{finish_reason}")
                break
            except requests.exceptions.ConnectionError:
                chunk = next(stream_response)  # å¤±è´¥äº†ï¼Œé‡è¯•ä¸€æ¬¡ï¼Ÿå†å¤±è´¥å°±æ²¡åŠæ³•äº†ã€‚
            response_text, reasoning_content,finish_reason = decode_chunk(chunk)
            # è¿”å›çš„æ•°æ®æµç¬¬ä¸€æ¬¡ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
            if reasoning_content == '' and response_text == "" and finish_reason != "False":
                continue
            if response_text == "API_ERROR" and (
                finish_reason != "False" or finish_reason != "stop"
            ):
                chunk = get_full_error(chunk, stream_response)
                chunk_decoded = chunk.decode()
                print(chunk_decoded)
                raise RuntimeError(
                    _('APIå¼‚å¸¸,è¯·æ£€æµ‹ç»ˆç«¯è¾“å‡ºã€‚å¯èƒ½çš„åŸå› æ˜¯: {}').format(finish_reason)
                )
            if chunk:
                try:
                    if finish_reason == "stop":
                        logging.info(f"[response] {result}")
                        break
                    if response_text:result += response_text
                    if observe_window is not None:
                        # è§‚æµ‹çª—ï¼ŒæŠŠå·²ç»è·å–çš„æ•°æ®æ˜¾ç¤ºå‡ºå»
                        if len(observe_window) >= 1:
                            observe_window[0] += response_text
                        # çœ‹é—¨ç‹—ï¼Œå¦‚æœè¶…è¿‡æœŸé™æ²¡æœ‰å–‚ç‹—ï¼Œåˆ™ç»ˆæ­¢
                        if len(observe_window) >= 2:
                            if (time.time() - observe_window[1]) > watch_dog_patience:
                                raise RuntimeError("ç”¨æˆ·å–æ¶ˆäº†ç¨‹åºã€‚")
                except Exception as e:
                    traceback.print_exc()
                    chunk = get_full_error(chunk, stream_response)
                    chunk_decoded = chunk.decode()
                    error_msg = chunk_decoded
                    print(error_msg)
                    raise RuntimeError(_("Jsonè§£æä¸åˆå¸¸è§„"))
        return result

    def predict(
        inputs,
        llm_kwargs,
        plugin_kwargs,
        chatbot,
        history=[],
        system_prompt="",
        stream=True,
        additional_fn=None,
    ):
        """
        å‘é€è‡³chatGPTï¼Œæµå¼è·å–è¾“å‡ºã€‚
        ç”¨äºåŸºç¡€çš„å¯¹è¯åŠŸèƒ½ã€‚
        inputs æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
        top_p, temperatureæ˜¯chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
        history æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨ï¼ˆæ³¨æ„æ— è®ºæ˜¯inputsè¿˜æ˜¯historyï¼Œå†…å®¹å¤ªé•¿äº†éƒ½ä¼šè§¦å‘tokenæ•°é‡æº¢å‡ºçš„é”™è¯¯ï¼‰
        chatbot ä¸ºWebUIä¸­æ˜¾ç¤ºçš„å¯¹è¯åˆ—è¡¨ï¼Œä¿®æ”¹å®ƒï¼Œç„¶åyeildå‡ºå»ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹å¯¹è¯ç•Œé¢å†…å®¹
        additional_fnä»£è¡¨ç‚¹å‡»çš„å“ªä¸ªæŒ‰é’®ï¼ŒæŒ‰é’®è§functional.py
        """
        lang = chatbot.get_language()
        _ = lambda text: init_language(text, lang)
        APIKEY = llm_kwargs['custom_api_key'](api_key_conf_name)

        if len(APIKEY) == 0:
            raise RuntimeError(_("APIKEYä¸ºç©º,è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„{}ã€‚æˆ–è€…å¯ä»¥è‡ªå®šä¹‰ {} API-KEY").format(api_key_conf_name,friendly_name))
        if inputs == "":
            inputs = "ä½ å¥½ğŸ‘‹"
        if additional_fn is not None:
            from core_functional import handle_core_functionality

            inputs, history = handle_core_functionality(
                additional_fn, inputs, history, chatbot
            )
        logging.info(f"[raw_input] {inputs}")
        chatbot.append((inputs, ""))
        yield from update_ui(
            chatbot=chatbot, history=history, msg=_("ç­‰å¾…å“åº”")
        )  # åˆ·æ–°ç•Œé¢

        # check mis-behavior
        if is_the_upload_folder(inputs):
            chatbot[-1] = (
                inputs,
                f"[Local Message] æ£€æµ‹åˆ°æ“ä½œé”™è¯¯ï¼å½“æ‚¨ä¸Šä¼ æ–‡æ¡£ä¹‹åï¼Œéœ€ç‚¹å‡»â€œ**å‡½æ•°æ’ä»¶åŒº**â€æŒ‰é’®è¿›è¡Œå¤„ç†ï¼Œè¯·å‹¿ç‚¹å‡»â€œæäº¤â€æŒ‰é’®æˆ–è€…â€œåŸºç¡€åŠŸèƒ½åŒºâ€æŒ‰é’®ã€‚",
            )
            yield from update_ui(
                chatbot=chatbot, history=history, msg="æ­£å¸¸"
            )  # åˆ·æ–°ç•Œé¢
            time.sleep(2)

        headers, playload = generate_message(
            input=inputs,
            model=llm_kwargs["llm_model"],
            key=APIKEY,
            history=history,
            max_output_token=max_output_token,
            system_prompt=system_prompt,
            temperature=llm_kwargs["temperature"],
        )

        history.append(inputs)
        history.append("")
        retry = 0
        while True:
            try:
                from .bridge_all import model_info

                endpoint = model_info[llm_kwargs["llm_model"]]["endpoint"]
                if not disable_proxy:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        proxies=proxies,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                else:
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        json=playload,
                        stream=True,
                        timeout=TIMEOUT_SECONDS,
                    )
                break
            except:
                retry += 1
                chatbot[-1] = (chatbot[-1][0], timeout_bot_msg)
                retry_msg = (
                    f", Retrying ({retry}/{MAX_RETRY}) â€¦â€¦" if MAX_RETRY > 0 else ""
                )
                yield from update_ui(
                    chatbot=chatbot, history=history, msg=_("è¯·æ±‚è¶…æ—¶") + retry_msg
                )  # åˆ·æ–°ç•Œé¢
                if retry > MAX_RETRY:
                    raise TimeoutError

        gpt_replying_buffer = ""
        gpt_reasoning_buffer = ''

        stream_response = response.iter_lines()
        while True:
            try:
                chunk = next(stream_response)
            except StopIteration:
                break
            except requests.exceptions.ConnectionError:
                chunk = next(stream_response)  # å¤±è´¥äº†ï¼Œé‡è¯•ä¸€æ¬¡ï¼Ÿå†å¤±è´¥å°±æ²¡åŠæ³•äº†ã€‚
            response_text,reasoning_content, finish_reason = decode_chunk(chunk)
            # è¿”å›çš„æ•°æ®æµç¬¬ä¸€æ¬¡ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
            if reasoning_content == '' and response_text == "" and finish_reason != "False":
                status_text = f"finish_reason: {finish_reason}"
                yield from update_ui(
                    chatbot=chatbot, history=history, msg=status_text
                )
                continue
            if chunk:
                try:
                    if response_text == "API_ERROR" and (
                        finish_reason != "False" or finish_reason != "stop"
                    ):
                        chunk = get_full_error(chunk, stream_response)
                        chunk_decoded = chunk.decode()
                        chatbot[-1] = (
                            chatbot[-1][0],
                            _("[Local Message] {},è·å¾—ä»¥ä¸‹æŠ¥é”™ä¿¡æ¯ï¼š\n").format(finish_reason)
                            + chunk_decoded,
                        )
                        yield from update_ui(
                            chatbot=chatbot,
                            history=history,
                            msg=_("APIå¼‚å¸¸: ") + chunk_decoded,
                        )  # åˆ·æ–°ç•Œé¢
                        print(chunk_decoded)
                        return

                    if finish_reason == "stop":
                        logging.info(f"[response] {gpt_replying_buffer}")
                        break

                    status_text = f"finish_reason: {finish_reason}"
                    if response_text:gpt_replying_buffer += response_text
                    if reasoning_content:gpt_reasoning_buffer += reasoning_content
                    # å¦‚æœè¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ï¼Œä¸€èˆ¬æ˜¯æ–‡æœ¬è¿‡é•¿ï¼Œè¯¦æƒ…è§get_full_errorçš„è¾“å‡º
                    history[-1] = gpt_replying_buffer

                    # å…¼å®¹æ·±åº¦æ€è€ƒ
                    if gpt_reasoning_buffer:
                        chatbot_assistant =HTML(''' 
                        <p>
                        <details open>
                        <summary>{}</summary>
                        <blockquote><p>
                        {}
                        </p></blockquote>
                        </details>  
                        </p>
                        {}
                        '''.format(_('æ·±åº¦æ€è€ƒ'),md2html(gpt_reasoning_buffer),md2html(gpt_replying_buffer))
                        )

                    else:chatbot_assistant = history[-1]

                    chatbot[-1] = (history[-2], chatbot_assistant)
                    yield from update_ui(
                        chatbot=chatbot, history=history, msg=status_text
                    )  # åˆ·æ–°ç•Œé¢
                except Exception as e:
                    yield from update_ui(
                        chatbot=chatbot, history=history, msg=_("Jsonè§£æä¸åˆå¸¸è§„")
                    )  # åˆ·æ–°ç•Œé¢
                    chunk = get_full_error(chunk, stream_response)
                    chunk_decoded = chunk.decode()
                    chatbot[-1] = (
                        chatbot[-1][0],
                        f"[Local Message] {_('è§£æé”™è¯¯,è·å¾—ä»¥ä¸‹æŠ¥é”™ä¿¡æ¯ï¼š')}\n" + chunk_decoded,
                    )
                    yield from update_ui(
                        chatbot=chatbot, history=history, msg=_("Jsonå¼‚å¸¸") + e
                    )  # åˆ·æ–°ç•Œé¢
                    print(chunk_decoded)
                    return

    return predict_no_ui_long_connection, predict
